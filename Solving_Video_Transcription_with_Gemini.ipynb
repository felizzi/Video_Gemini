{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felizzi/Video_Gemini/blob/main/Solving_Video_Transcription_with_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtxoQixAqoNu"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPRBl_orqoNv"
      },
      "source": [
        "# Solving Video Transcription with Gemini\n",
        "\n",
        "|           |                                                  |\n",
        "| --------- | ------------------------------------------------ |\n",
        "| Author(s) | [Laurent Picard](https://github.com/PicardParis) |\n",
        "\n",
        "> ![Lab Preview](https://github.com/PicardParis/cherry-on-py-pics/raw/main/misc/Solving-Video-Transcription-with-Gemini.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwKb-__qK02C"
      },
      "source": [
        "---\n",
        "\n",
        "## üî• Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35E-CpC6qoNw"
      },
      "source": [
        "To fully transcribe a video, we're looking to answer the following questions:\n",
        "\n",
        "- 1Ô∏è‚É£ What was said and when?\n",
        "- 2Ô∏è‚É£ Who are the speakers?\n",
        "- 3Ô∏è‚É£ Who said what?\n",
        "\n",
        "Can we solve this problem in a straightforward and efficient way?\n",
        "\n",
        "In other words, consider this challenge: Can we transcribe any video with just the following?\n",
        "\n",
        "- 1 video\n",
        "- 1 prompt\n",
        "- 1 request\n",
        "\n",
        "Let's try with Gemini‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oArL-WR6qoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## üåü State of the art"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqIf_8wqoNx"
      },
      "source": [
        "### 1Ô∏è‚É£ What was said and when?\n",
        "\n",
        "This is a known problem with a known solution:\n",
        "\n",
        "- **Speech-to-text** (STT) is a process that takes an audio input and transforms speech into text. STT can provide timestamps at the word level. It's also known as automatic speech recognition (ASR).\n",
        "\n",
        "In the last decade, it's been best addressed by task-specific machine learning (ML) models.\n",
        "\n",
        "### 2Ô∏è‚É£ Who are the speakers?\n",
        "\n",
        "We can retrieve speaker names in a video from two sources:\n",
        "\n",
        "- **What's written** (e.g., speakers can be introduced with an on-screen information when they first speak)\n",
        "- **What's spoken** (e.g., \"Hello Bob! Alice, how are you doing?\")\n",
        "\n",
        "Vision and natural-language-processing (NLP) models can help with the following features:\n",
        "\n",
        "- Vision: **Optical character recognition** (OCR), also called text detection, extracts the text visible in images.\n",
        "- Vision: **Person detection** lets you know if and where there are persons in an image.\n",
        "- NLP: **Entity extraction** can identify named entities in text.\n",
        "\n",
        "### 3Ô∏è‚É£ Who said what?\n",
        "\n",
        "This is another known problem with a partial solution (complementary to speech-to-text):\n",
        "\n",
        "- **Speaker diarization** (also known as speaker turn segmentation) is a process that splits an audio stream into segments for the different detected speakers (\"Speaker A\", \"Speaker B\", etc.).\n",
        "\n",
        "Researchers have worked in this field for decades and, though machine learning (ML) models brought significant progress in the past years, this is still a very active field of research. Existing solutions come with some shortcomings as they generally require providing hints for the audio inputs (notably the language spoken and the number of speakers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5D5Mrt9qoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## üí° A new problem-solving tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0qJrzzRqoNx"
      },
      "source": [
        "Solving all of 1Ô∏è‚É£, 2Ô∏è‚É£, and 3Ô∏è‚É£ is really not obvious. This would probably involve setting up an elaborate supervised processing pipeline with a few state-of-the-art ML models. Additionally, as of early 2025, our challenge (advanced video transcription) doesn't look like a solved problem, so we may need days or weeks to set up such a pipeline, without any certainty to reach a viable solution.\n",
        "\n",
        "### üé¨ Multimodal\n",
        "\n",
        "Gemini is a natively multimodal, which means it can process the following inputs:\n",
        "\n",
        "- text\n",
        "- audio\n",
        "- images\n",
        "- videos\n",
        "- documents\n",
        "\n",
        "### üåê Multilingual\n",
        "\n",
        "Gemini is also [multilingual](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#languages-gemini):\n",
        "\n",
        "- It can process inputs and generate outputs in 100+ languages\n",
        "- If we manage to reach a solution, this should also work for videos in different languages\n",
        "\n",
        "### üß∞ A natural-language toolbox\n",
        "\n",
        "Gemini allows for rapid prompt-based problem solving. With just text instructions, we can extract information and transform it into new information, in a straightforward and automated workflow. This lets us shift from relying on task-specific ML models to using a versatile large language model (LLM).\n",
        "\n",
        "Being both multimodal and multilingual, Gemini lets us solve complex problems using natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0_VsUthqoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## üèÅ Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wbID7ORqoNx"
      },
      "source": [
        "### üêç Python packages\n",
        "\n",
        "We'll use the following packages:\n",
        "\n",
        "- `google-genai`: the [Google Gen AI Python SDK](https://pypi.org/project/google-genai) lets us call Gemini with a few lines of code\n",
        "- `pandas` for data visualization\n",
        "- `tenacity` for request management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZBN80r7qtgs"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet \"google-genai>=1.2.0\" \"pandas[output-formatting]\" tenacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL3o450NqoNy"
      },
      "source": [
        "### üîë Authentication (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD2E2dh9qoNz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "    colab_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGgmHVdQqoNz"
      },
      "source": [
        "### ‚öôÔ∏è Google Cloud settings\n",
        "\n",
        "In this notebook, we'll use Vertex AI to send requests to Gemini.\n",
        "\n",
        "To get started using Vertex AI, here are the requirements:\n",
        "- An existing Google Cloud project\n",
        "- The [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) must be enabled\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTov81qlqoNz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ‚öôÔ∏è Project\n",
        "GOOGLE_CLOUD_PROJECT = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "if not GOOGLE_CLOUD_PROJECT:\n",
        "    # Retrieve from environment (Colab Enterprise or Vertex AI Workbench)\n",
        "    GOOGLE_CLOUD_PROJECT = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"\")\n",
        "assert GOOGLE_CLOUD_PROJECT, \"GOOGLE_CLOUD_PROJECT is not defined\"\n",
        "\n",
        "# ‚öôÔ∏è API location\n",
        "# Update if needed, see https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations\n",
        "GOOGLE_CLOUD_REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEYSAFqrqoNz"
      },
      "source": [
        "### ü§ñ Gen AI SDK client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR77aUhzqoNz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "genai_client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=GOOGLE_CLOUD_PROJECT,\n",
        "    location=GOOGLE_CLOUD_REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vk9e4V_qoNz"
      },
      "source": [
        "### üß† Gemini model & configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acdi0aoIqoNz"
      },
      "source": [
        "Gemini comes in different [versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n",
        "\n",
        "Let's pick Gemini 2.0 Flash which offers both high performances and low latency:\n",
        "\n",
        "```python\n",
        "GEMINI_2_0_FLASH = \"gemini-2.0-flash-001\"\n",
        "```\n",
        "\n",
        "Gemini can be used in different ways, ranging from factual to creative mode. The problem we're trying to solve is a **data extraction** use case. We want results as factual and deterministic as possible. For this, we can change the [content generation parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters).\n",
        "\n",
        "Let's set the `temperature` and `seed` parameters to minimize randomness:\n",
        "\n",
        "```python\n",
        "from google.genai.types import GenerateContentConfig\n",
        "\n",
        "DEFAULT_CONFIG = GenerateContentConfig(\n",
        "    temperature=0.0,\n",
        "    seed=42,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUBe2u8IqoNz"
      },
      "source": [
        "### üõ†Ô∏è Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bus2ODLIK02F"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "from dataclasses import dataclass\n",
        "from datetime import timedelta\n",
        "\n",
        "import IPython.display\n",
        "import tenacity\n",
        "from google.genai.errors import ClientError\n",
        "from google.genai.types import (\n",
        "    FileData,\n",
        "    FinishReason,\n",
        "    GenerateContentConfig,\n",
        "    GenerateContentResponse,\n",
        "    Part,\n",
        "    VideoMetadata,\n",
        ")\n",
        "\n",
        "\n",
        "class Model(enum.Enum):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_0_FLASH = \"gemini-2.0-flash-001\"\n",
        "    GEMINI_2_0_FLASH_LITE = \"gemini-2.0-flash-lite-001\"\n",
        "    GEMINI_1_5_PRO = \"gemini-1.5-pro-002\"\n",
        "    # Preview or Experimental\n",
        "    GEMINI_2_0_PRO = \"gemini-2.0-pro-exp-02-05\"\n",
        "    # Default model\n",
        "    DEFAULT = GEMINI_2_0_FLASH\n",
        "\n",
        "\n",
        "# Default configuration for more deterministic outputs\n",
        "DEFAULT_CONFIG = GenerateContentConfig(\n",
        "    temperature=0.0,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "YOUTUBE_URL_PREFIX = \"https://www.youtube.com/watch?v=\"\n",
        "\n",
        "\n",
        "def youtube_url_from_id(youtube_id: str) -> str:\n",
        "    return f\"{YOUTUBE_URL_PREFIX}{youtube_id}\"\n",
        "\n",
        "\n",
        "class Video(enum.Enum):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TestVideo(Video):\n",
        "    GDM_PODCAST_TRAILER_0MIN_59S = youtube_url_from_id(\"0pJn3g8dfwk\")\n",
        "    JANE_GOODALL_2MIN_42S = \"gs://cloud-samples-data/video/JaneGoodall.mp4\"\n",
        "    GDM_ALPHAFOLD_7MIN_54S = youtube_url_from_id(\"gg7WjuFs8F4\")\n",
        "    BRUT_FR_DOGS_WATER_LEAK_8MIN_28S = youtube_url_from_id(\"U_yYkb-ureI\")\n",
        "    GDM_VIEW_FROM_FRONTIER_34MIN_38S = youtube_url_from_id(\"SM_vpRtg2Ac\")\n",
        "\n",
        "\n",
        "class ShowResponseAs(enum.Enum):\n",
        "    DONT_SHOW = enum.auto()\n",
        "    TEXT = enum.auto()\n",
        "    MARDKDOWN = enum.auto()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VideoSegment:\n",
        "    start: timedelta\n",
        "    end: timedelta\n",
        "\n",
        "\n",
        "@tenacity.retry(\n",
        "    retry=tenacity.retry_if_exception_type(ClientError),\n",
        "    wait=tenacity.wait_fixed(30),\n",
        "    stop=tenacity.stop_after_attempt(5),\n",
        "    reraise=True,\n",
        ")\n",
        "def generate_content(\n",
        "    prompt: str,\n",
        "    video: Video | None = None,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    model: Model = Model.DEFAULT,\n",
        "    config: GenerateContentConfig = DEFAULT_CONFIG,\n",
        "    show_as: ShowResponseAs = ShowResponseAs.TEXT,\n",
        ") -> None:\n",
        "    model_id = model.value\n",
        "    prompt = prompt.strip()\n",
        "    if video:\n",
        "        contents = [content_part_from_video(video, video_segment), prompt]\n",
        "        caption = f\"{video.name} / {model_id}\"\n",
        "    else:\n",
        "        contents = prompt\n",
        "        caption = f\"{model_id}\"\n",
        "    print(f\" {caption} \".center(80, \"-\"))\n",
        "\n",
        "    response = genai_client.models.generate_content(\n",
        "        model=model_id,\n",
        "        contents=contents,\n",
        "        config=config,\n",
        "    )\n",
        "    show_response(response, show_as)\n",
        "\n",
        "\n",
        "def content_part_from_video(\n",
        "    video: Video,\n",
        "    video_segment: VideoSegment | None = None,\n",
        ") -> Part:\n",
        "    def str_offset(offset: timedelta) -> str:\n",
        "        return f\"{offset.total_seconds():.0f}s\"\n",
        "\n",
        "    file_data = FileData(file_uri=video.value, mime_type=\"video/*\")\n",
        "    video_metadata = (\n",
        "        None\n",
        "        if video_segment is None\n",
        "        else VideoMetadata(\n",
        "            start_offset=str_offset(video_segment.start),\n",
        "            end_offset=str_offset(video_segment.end),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return Part(file_data=file_data, video_metadata=video_metadata)\n",
        "\n",
        "\n",
        "def show_response(response: GenerateContentResponse, show_as: ShowResponseAs) -> None:\n",
        "    if show_as == ShowResponseAs.DONT_SHOW:\n",
        "        return\n",
        "    if not response.candidates:\n",
        "        print(\"‚ùå No `response.candidates`\")\n",
        "        return\n",
        "    if response.candidates[0].finish_reason != FinishReason.STOP:\n",
        "        print(f\"‚ùå {response.candidates[0].finish_reason = }\")\n",
        "    if not (response_text := response.text):\n",
        "        print(\"‚ùå No `response.text`\")\n",
        "        return\n",
        "    response_text = response_text.strip()\n",
        "\n",
        "    match show_as:\n",
        "        case ShowResponseAs.TEXT:\n",
        "            print(response_text)\n",
        "        case ShowResponseAs.MARDKDOWN:\n",
        "            display_markdown(response_text)\n",
        "\n",
        "\n",
        "def display_markdown(markdown: str) -> None:\n",
        "    IPython.display.display(IPython.display.Markdown(markdown))\n",
        "\n",
        "\n",
        "def display_video(video: Video) -> None:\n",
        "    video_url = video.value\n",
        "    if video_url.startswith(\"gs://\"):\n",
        "        cloud_storage_path = video_url.removeprefix(\"gs://\")\n",
        "        video_url = f\"https://storage.googleapis.com/{cloud_storage_path}\"\n",
        "    assert video_url.startswith(\"https://\")\n",
        "\n",
        "    video_width = 600\n",
        "    if video_url.startswith(YOUTUBE_URL_PREFIX):\n",
        "        youtube_id = video_url.removeprefix(YOUTUBE_URL_PREFIX)\n",
        "        ipython_video = IPython.display.YouTubeVideo(youtube_id, width=video_width)\n",
        "    else:\n",
        "        ipython_video = IPython.display.Video(video_url, width=video_width)\n",
        "\n",
        "    display_markdown(f\"## Video ([source]({video_url}))\")\n",
        "    IPython.display.display(ipython_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B499IGYqoN0"
      },
      "source": [
        "---\n",
        "\n",
        "## üß™ Prototyping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbw8w44lqoN0"
      },
      "source": [
        "### üå± Natural behavior\n",
        "\n",
        "Before diving any deeper, it's interesting to see how Gemini responds to simple instructions, to develop some intuition about its natural behavior.\n",
        "\n",
        "Let's first check what we get with minimalistic prompts and a short English video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0AZM7CoqoN0"
      },
      "outputs": [],
      "source": [
        "video = TestVideo.GDM_PODCAST_TRAILER_0MIN_59S\n",
        "display_video(video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF3MCXLTqoN0"
      },
      "outputs": [],
      "source": [
        "prompt = \"Transcribe the video\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jw70MYlqoN0"
      },
      "source": [
        "Results:\n",
        "- Gemini naturally outputs a list of `[timecode] transcript` lines.\n",
        "- That's speech-to-text in a one-liner!\n",
        "- It looks like we can answer 1Ô∏è‚É£ \"What what said and when?\".\n",
        "\n",
        "Now, what about 2Ô∏è‚É£ \"Who are the speakers?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOaSzf4TqoN0"
      },
      "outputs": [],
      "source": [
        "prompt = \"List the people visible in the video\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPX61D20qoN0"
      },
      "source": [
        "Results:\n",
        "\n",
        "- Gemini is able to consolidate the names visible on title cards during the video.\n",
        "- That's OCR + entity extraction in a one-liner!\n",
        "- 2Ô∏è‚É£ \"Who are the speakers?\" looks to be solved too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtQ1vNiqoN0"
      },
      "source": [
        "### ‚è© Not so fast!\n",
        "\n",
        "Then, the next natural reflex is to jump to final instructions to solve our problem once and for all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF6BM_HfqoN1"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Transcribe the video, including speaker names (\"?\" if not found).\n",
        "\n",
        "Format example:\n",
        "[00:02] John Doe: Hello Alice!\n",
        "\"\"\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I68y1ZBUqoN1"
      },
      "source": [
        "This is almost fully correct: The first transcript is not attributed to the host, but everything else looks correct.\n",
        "\n",
        "Nonetheless, we're not in real conditions:\n",
        "\n",
        "- The video is very short (less than a minute)\n",
        "- The video is also very simple (speakers alternate and are introduced by title cards)\n",
        "\n",
        "Let's try with this 8 minute (and more complex) video:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycy--DT4qoN1"
      },
      "outputs": [],
      "source": [
        "generate_content(prompt, TestVideo.GDM_ALPHAFOLD_7MIN_54S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh0XSXOtqoN1"
      },
      "source": [
        "This falls apart: Most transcripts have no speaker!\n",
        "\n",
        "At this stage:\n",
        "\n",
        "- We might conclude that we can't solve the problem with real-life videos.\n",
        "- Persevering in trying more and more elaborate prompts for this unsolved problem may result in a waste of time.\n",
        "\n",
        "Let's take a step back and think about what happens under the hood‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "784a-K1wqoN1"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚öõÔ∏è Under the hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2gSrBCPqoN1"
      },
      "source": [
        "### ü™ô Tokens\n",
        "\n",
        "Tokens are the LLMs' building blocks. A token represents a piece of information.\n",
        "\n",
        "Examples of Gemini multimodal tokens:\n",
        "\n",
        "| content            | #tokens        | details                                |\n",
        "| ------------------ | -------------- | -------------------------------------- |\n",
        "| `hello`            | 1              | 1 token for common words/sequences     |\n",
        "| `enthusiastic`     | 2              | `enthusi‚Ä¢astic`                        |\n",
        "| `enthusiastically` | 3              | `enthusi‚Ä¢astic‚Ä¢ally`                   |\n",
        "| image              | 258            |                                        |\n",
        "| audio              | 32 per second  | Managed by the audio tokenizer         |\n",
        "| video              | 263 per second | Sampled by the video tokenizer (1 fps) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzkrIwznqoN1"
      },
      "source": [
        "### üßÆ Probabilities all the way down\n",
        "\n",
        "The ability of LLMs to exchange in flawless natural language is very impressive, but it's easy to get carried away and reach wrong assumptions.\n",
        "\n",
        "Keep in mind how LLMs work:\n",
        "\n",
        "- LLMs are trained on massive tokenized datasets: this represents the LLM knowledge\n",
        "- During the training, their neural network learns token patterns\n",
        "- When you send a request to an LLM, your inputs are transformed into tokens\n",
        "- To answer your request, the LLM predicts, token by token, the next likely tokens\n",
        "- Overall, LLMs are exceptionnal statistical token prediction machines, but nothing more\n",
        "\n",
        "This has a few consequences:\n",
        "\n",
        "- LLM outputs are just a logical follow-up to your inputs (based on the LLM knowledge)\n",
        "- LLMs seem to be able to reason but it's just an appearance; they have no real understanding\n",
        "- LLMs have no awareness: they learnt patterns but are completely ignorant of their inner workings\n",
        "- LLMs have no conscience: they are designed to generate tokens and will do so based on your instructions\n",
        "- Order matters: Tokens that are generated first will influence tokens that are generated next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bN1DcQqoN2"
      },
      "source": [
        "For the next step, some methodical prompt engineering might help‚Ä¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w2owFhrqoN2"
      },
      "source": [
        "---\n",
        "\n",
        "## üèóÔ∏è Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3lIFIj5qoN2"
      },
      "source": [
        "### ü™ú Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMklaeV3qoN2"
      },
      "source": [
        "Prompt engineering is still a pretty recent field. It involves designing and refining text instructions to guide LLMs towards generating desired outputs. Like writing, it is both art and science, a skill everyone can develop with practice and discipline.\n",
        "\n",
        "We can find countless reference materials about prompt engineering. For some of them, the prompts are very long, complex, and scary. Crafting prompts with a highly performant LLM like Gemini is a lot more simple. Here are key adjectives we can keep in mind:\n",
        "\n",
        "- iterative\n",
        "- precise\n",
        "- concise\n",
        "\n",
        "**Iterative**\n",
        "\n",
        "Prompt engineering is typically a very iteractive process. Here are some recommendations:\n",
        "\n",
        "- Craft your prompt step by step\n",
        "- Keep track of your successive iterations\n",
        "- At every iteration, make sure to measure what's working vs. not working\n",
        "- If you reach a regression, backtrack to a successful iteration\n",
        "\n",
        "**Precise**\n",
        "\n",
        "Precision is key:\n",
        "\n",
        "- Use words as specific as possible\n",
        "- Words with different meanings can introduce variability, so use precise expressions\n",
        "- Precision will influence probabilities in your favor\n",
        "\n",
        "**Concise**\n",
        "\n",
        "Concision has additional advantages:\n",
        "\n",
        "- A short prompt is more straightforward to understand (and maintain!) for us\n",
        "- The longer your prompt is, the more likely you are to introduce inconsistencies or even contradictions, resulting in variable interpretations of your instructions\n",
        "- Test and trust the LLM knowledge: it's part of your context and can make your prompt shorter\n",
        "\n",
        "Overall, this may seem contradictory but, if you take the time to be iterative, precise, and concise, you are likely to save a lot of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQOEU3wLqoN2"
      },
      "source": [
        "### üìö Terminology\n",
        "\n",
        "We're not experts in video transcription but we want Gemini to behave as one. Consequently, we'd like to write prompts as specific as possible to this use case. If LLMs can only understand instructions that are part of their training knowledge, they can also share this knowledge with us.\n",
        "\n",
        "We can learn a lot by directly asking Gemini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Q6dKvRqoN2"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "What is the terminology used for video transcriptions?\n",
        "Please show a typical output example.\n",
        "\"\"\"\n",
        "generate_content(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KTjxDPoqoN2"
      },
      "source": [
        "### üìù Strategy\n",
        "\n",
        "So far, we've seen the following:\n",
        "\n",
        "- We did not manage to get the full transcription with identified speakers all at once\n",
        "- Order matters (because a generated token will influence the probabilities for the next tokens)\n",
        "\n",
        "To tackle our challenge, we need Gemini to infer from the following multimodal information:\n",
        "\n",
        "- text (our instructions + what may be written in the video)\n",
        "- audio (what's heard in the video)\n",
        "- visual (what's visible in the video)\n",
        "- time (when things happen)\n",
        "\n",
        "That is quite a bunch of mixed types of information!\n",
        "\n",
        "Video transcription is a data extraction use case, which can be seen as creating a database. If we follow this logic, prompt engineering can then be seen as creating a database with related tables. In addition, our final goal is to have an automated workflow, so we can start reasoning in terms of JSON fields.\n",
        "\n",
        "Let's split our instructions into steps (tables) and in a meaningful order‚Ä¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fw5nQJ9qoN2"
      },
      "source": [
        "### üí¨ Transcripts\n",
        "\n",
        "First of all, let's focus on getting the transcripts:\n",
        "\n",
        "- It is central and independent information.\n",
        "- Gemini showed to be natively good at it.\n",
        "\n",
        "We've also seen what a typical transcription entry can look like:\n",
        "\n",
        "```\n",
        "00:02 speaker_1: Welcome!\n",
        "```\n",
        "\n",
        "But, right away, there can be some ambiguities in our multimodal use case:\n",
        "\n",
        "- What is a speaker?\n",
        "- Is it someone we see/hear?\n",
        "- What if the person visible in the video is not the one speaking?\n",
        "- What if the person speaking is never seen in the video?\n",
        "\n",
        "How do we inconsciously identify who's speaking in a video?\n",
        "\n",
        "- Probably first by identifying the different voices on the fly\n",
        "- Then probably by consolidating additional audio and visual cues\n",
        "\n",
        "Is Gemini able to understand voice characteristics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZpvtyEZqoN2"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "List the following characteristics audible in the provided video:\n",
        "- Voice pitches\n",
        "- Accents\n",
        "- Languages\n",
        "\"\"\"\n",
        "generate_content(prompt, TestVideo.GDM_PODCAST_TRAILER_0MIN_59S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRy_zzFHqoN3"
      },
      "source": [
        "What about a French video?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6bb7r_SqoN3"
      },
      "outputs": [],
      "source": [
        "generate_content(prompt, TestVideo.BRUT_FR_DOGS_WATER_LEAK_8MIN_28S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19qIqFbdqoN3"
      },
      "source": [
        "‚ö†Ô∏è We have to be cautious about how we interpret the responses: they can consolidate multimodal info or even common knowledge. For example, if a person is famously known to be from the UK, a possible inference can be that they have a British accent.\n",
        "\n",
        "Nonetheless, if you do more tests, especially on private content (not part of common knowledge), it looks like Gemini's audio tokenizer does wonders and extracts speech semantic info!\n",
        "\n",
        "After a few iterations, we can reach a transcription prompt focusing on the audio and on voices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWDIQMNmqoN3"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Transcripts\n",
        "- Transcribe the video's audio.\n",
        "  - Split overlapping speech into different transcript entries.\n",
        "  - Include start timecodes (MM:SS) and verbatim transcripts.\n",
        "  - Identify matching voices and label each voice with a unique ID (voice_1, voice_2‚Ä¶).\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `timecode`\n",
        "  - `verbatim`\n",
        "  - `voice_id`\n",
        "\"\"\"\n",
        "generate_content(prompt, TestVideo.GDM_PODCAST_TRAILER_0MIN_59S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxiB4dK3qoN3"
      },
      "source": [
        "This is looking good! And if you test the instructions on more complex videos, you'll get similar promising results.\n",
        "\n",
        "Notice how the prompt reuses cherry-picked terms from the previously requested terminology, while aiming for precision and concision:\n",
        "\n",
        "- `timecode` is specific (`timestamp` has more meanings)\n",
        "- `MM:SS` clarifies the timecode format\n",
        "- `verbatim` is unambiguous (\"spoken words\" has more meanings)\n",
        "- `voice_1, voice_2‚Ä¶` is an ellipse but we're trusting Gemini's pattern abilities\n",
        "\n",
        "We're half way. Let's complete our database generation with a second step‚Ä¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjSoAYomqoN3"
      },
      "source": [
        "### üßë Speakers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4JRfMpOqoN3"
      },
      "source": [
        "The second step is pretty straightforward. We want to extract speaker information in a second table. The two tables are logically linked by the voice ID.\n",
        "\n",
        "After a few iterations, we can reach a two-step prompt such as the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ysk17GqoN3"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Step 1 - Transcripts\n",
        "- Transcribe the video's audio.\n",
        "  - Split overlapping speech into different transcript entries.\n",
        "  - Include start timecodes (MM:SS) and verbatim transcripts.\n",
        "  - Identify matching voices and label each voice with a unique ID (voice_1, voice_2‚Ä¶).\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `timecode`\n",
        "  - `verbatim`\n",
        "  - `voice_id`\n",
        "\n",
        "Step 2 - Speakers\n",
        "- For each `voice_id` from Step 1, extract information about the speaker.\n",
        "  - Only use information explicitly stated in the video (use \"n/a\" otherwise).\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `voice_id`\n",
        "  - `name`\n",
        "  - `role`\n",
        "\"\"\"\n",
        "generate_content(prompt, TestVideo.GDM_PODCAST_TRAILER_0MIN_59S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGslUhnAqoN3"
      },
      "source": [
        "Test the prompt on more complex videos: This keeps looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68a20ekqoN4"
      },
      "source": [
        "---\n",
        "\n",
        "## üß© Structured output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hHAYumqoN4"
      },
      "source": [
        "We've iterated towards a precise and concise prompt. At this stage, we can focus on Gemini's response:\n",
        "\n",
        "- It is a plain text response with fenced code blocks\n",
        "- Instead, we'd like to get a structured output, so we receive consistently formatted responses\n",
        "- Ideally, we'd also like to avoid having to parse the response (a maintenance burden)\n",
        "\n",
        "Getting structured outputs is a feature also called \"controlled generation\". As we've already crafted our prompt in terms of data tables and JSON fields, this is now a formality. In our request, we can add the following parameters:\n",
        "\n",
        "- `response_mime_type=\"application/json\"`\n",
        "- `response_schema=\"YOUR_JSON_SCHEMA\"` ([see doc](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output#fields))\n",
        "\n",
        "In Python, this gets even easier:\n",
        "\n",
        "- Use the `pydantic` library\n",
        "- Reflect your prompt structure with classes derived from `pydantic.BaseModel`\n",
        "\n",
        "Our prompt (unchanged):\n",
        "\n",
        "```txt\n",
        "Step 1 - Transcripts\n",
        "‚Ä¶\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `timecode`\n",
        "  - `verbatim`\n",
        "  - `voice_id`\n",
        "\n",
        "Step 2 - Speakers\n",
        "‚Ä¶\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `voice_id`\n",
        "  - `name`\n",
        "  - `role`\n",
        "```\n",
        "\n",
        "The corresponding Python classes:\n",
        "\n",
        "```python\n",
        "import pydantic\n",
        "\n",
        "class Transcript(pydantic.BaseModel):\n",
        "    timecode: str\n",
        "    verbatim: str\n",
        "    voice_id: str\n",
        "\n",
        "class Speaker(pydantic.BaseModel):\n",
        "    voice_id: str\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "class VideoTranscription(pydantic.BaseModel):\n",
        "    step1_transcripts: list[Transcript] = pydantic.Field(default_factory=list)\n",
        "    step2_speakers: list[Speaker] = pydantic.Field(default_factory=list)\n",
        "```\n",
        "\n",
        "Sending a request to Gemini:\n",
        "\n",
        "```python\n",
        "response = genai_client.models.generate_content(\n",
        "    # ‚Ä¶\n",
        "    config=GenerateContentConfig(\n",
        "        # ‚Ä¶\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=VideoTranscription,\n",
        "        # ‚Ä¶\n",
        "    ),\n",
        ")\n",
        "```\n",
        "\n",
        "Using the response from Gemini:\n",
        "\n",
        "```python\n",
        "if isinstance(response.parsed, VideoTranscription):\n",
        "    video_transcription = response.parsed\n",
        "else:\n",
        "    video_transcription = VideoTranscription()  # Empty transcription\n",
        "```\n",
        "\n",
        "What's interesting with this approach:\n",
        "\n",
        "- We don't need to change our prompt when using a response schema\n",
        "- It's easy to change/maintain both prompt and classes in the same location\n",
        "- The JSON schema is automatically generated from the class hierarchy and dispatched to Gemini\n",
        "- The response is automatically parsed and serialized into the corresponding Python objects\n",
        "\n",
        "To put words into action, let's add a `company` field for the speakers and finalize our code‚Ä¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RePBxIKqqoN4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import pydantic\n",
        "from google.genai.types import MediaResolution\n",
        "\n",
        "\n",
        "UNKNOWN_DATA = \"unknown\"\n",
        "\n",
        "VIDEO_TRANSCRIPTION_PROMPT = f\"\"\"\n",
        "Step 1 - Transcripts\n",
        "- Task:\n",
        "  - Transcribe the video's audio verbatim, including all spoken words.\n",
        "  - Include start timecodes for each turn in MM:SS format.\n",
        "  - A \"turn\" is defined as everything heard from a single voice before another voice starts speaking.\n",
        "  - Separate overlapping speech into distinct turns.\n",
        "  - Assign a unique identifier to each distinct voice (voice_1, voice_2‚Ä¶).\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `timecode`\n",
        "  - `verbatim`\n",
        "  - `voice_id`\n",
        "\n",
        "Step 2 - Speakers\n",
        "- Task:\n",
        "  - For each `voice_id` from Step 1, extract information about the speaker.\n",
        "  - Only use information explicitly stated in the video (use \"{UNKNOWN_DATA}\" otherwise).\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `voice_id`\n",
        "  - `name`\n",
        "  - `role`\n",
        "  - `company`\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Transcript(pydantic.BaseModel):\n",
        "    timecode: str\n",
        "    verbatim: str\n",
        "    voice_id: str\n",
        "\n",
        "\n",
        "class Speaker(pydantic.BaseModel):\n",
        "    voice_id: str\n",
        "    name: str\n",
        "    role: str\n",
        "    company: str\n",
        "\n",
        "\n",
        "class VideoTranscription(pydantic.BaseModel):\n",
        "    step1_transcripts: list[Transcript] = pydantic.Field(default_factory=list)\n",
        "    step2_speakers: list[Speaker] = pydantic.Field(default_factory=list)\n",
        "\n",
        "\n",
        "def video_transcription_from_response(\n",
        "    response: GenerateContentResponse,\n",
        ") -> VideoTranscription:\n",
        "    empty_transcription = VideoTranscription()\n",
        "\n",
        "    if not response.candidates:\n",
        "        print(f\"‚ùå No `response.candidates`\")\n",
        "        return empty_transcription\n",
        "\n",
        "    if response.candidates[0].finish_reason != FinishReason.STOP:\n",
        "        print(f\"‚ùå {response.candidates[0].finish_reason = }\")\n",
        "        return empty_transcription\n",
        "\n",
        "    if not isinstance(response.parsed, VideoTranscription):\n",
        "        print(\"‚ùå Could not parse the JSON response\")\n",
        "        return empty_transcription\n",
        "\n",
        "    return response.parsed\n",
        "\n",
        "\n",
        "def media_resolution_for_video(video: Video) -> MediaResolution:\n",
        "    if not (match := re.search(r\"_(\\d+)MIN_(\\d+)S$\", video.name)):\n",
        "        print(\n",
        "            f\"‚ö†Ô∏è No duration info in video enum: {video.name} (expected end: _?MIN_?S)\"\n",
        "        )\n",
        "        return MediaResolution.MEDIA_RESOLUTION_MEDIUM\n",
        "\n",
        "    # Arbitrary heuristic: reduce prevalence of video tokens for 5min+ videos\n",
        "    if 60 * 5 <= 60 * int(match.group(1)) + int(match.group(2)):\n",
        "        return MediaResolution.MEDIA_RESOLUTION_LOW\n",
        "    else:\n",
        "        return MediaResolution.MEDIA_RESOLUTION_MEDIUM\n",
        "\n",
        "\n",
        "@tenacity.retry(\n",
        "    retry=tenacity.retry_if_exception_type(ClientError),\n",
        "    wait=tenacity.wait_fixed(30),\n",
        "    stop=tenacity.stop_after_attempt(5),\n",
        "    reraise=True,\n",
        ")\n",
        "def get_video_transcription(\n",
        "    video: Video,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    prompt: str = VIDEO_TRANSCRIPTION_PROMPT,\n",
        "    model: Model = Model.DEFAULT,\n",
        ") -> VideoTranscription:\n",
        "    model_name = model.value\n",
        "    contents = [content_part_from_video(video, video_segment), prompt.strip()]\n",
        "    config = GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        seed=42,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=VideoTranscription,\n",
        "        media_resolution=media_resolution_for_video(video),\n",
        "    )\n",
        "\n",
        "    print(f\" {video.name} / {model_name} \".center(80, \"-\"))\n",
        "    response = genai_client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=contents,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    return video_transcription_from_response(response)\n",
        "\n",
        "\n",
        "print(VIDEO_TRANSCRIPTION_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5s3EKrKqoN4"
      },
      "source": [
        "Let's test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0_pIQVHqoN4"
      },
      "outputs": [],
      "source": [
        "transcription = get_video_transcription(TestVideo.GDM_PODCAST_TRAILER_0MIN_59S)\n",
        "\n",
        "print(f\"# Transcripts: {len(transcription.step1_transcripts)}\")\n",
        "print(f\"# Speakers:    {len(transcription.step2_speakers)}\")\n",
        "for speaker in transcription.step2_speakers:\n",
        "    print(f\"  - {speaker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa1MrRKsqoN4"
      },
      "source": [
        "---\n",
        "\n",
        "## üìä Data visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcGct03AqoN4"
      },
      "source": [
        "We started prototyping in natural language, crafted a prompt, and generated a structured output. As reading raw data can be painful, we can now present video transcriptions in a more pleasant manner.\n",
        "\n",
        "Here's a possible orchestrator function:\n",
        "\n",
        "```python\n",
        "def transcribe_video(video: Video):\n",
        "    display_video(video)\n",
        "    transcription = get_video_transcription(video)\n",
        "    display_speakers(transcription)\n",
        "    display_transcripts(transcription)\n",
        "```\n",
        "\n",
        " Let's add some data visualization functions‚Ä¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5u23XdeqoN4"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "from pandas import DataFrame, Series\n",
        "from pandas.io.formats.style import Styler\n",
        "from pandas.io.formats.style_render import CSSDict\n",
        "\n",
        "\n",
        "def yield_known_speaker_color() -> Iterator[str]:\n",
        "    COLS_40 = (\"#669DF6\", \"#EE675C\", \"#FCC934\", \"#5BB974\")\n",
        "    COLS_30 = (\"#8AB4F8\", \"#F28B82\", \"#FDD663\", \"#81C995\")\n",
        "    COLS_20 = (\"#AECBFA\", \"#F6AEA9\", \"#FDE293\", \"#A8DAB5\")\n",
        "    COLS_10 = (\"#D2E3FC\", \"#FAD2CF\", \"#FEEFC3\", \"#CEEAD6\")\n",
        "    COLS_05 = (\"#E8F0FE\", \"#FCE8E6\", \"#FEF7E0\", \"#E6F4EA\")\n",
        "    while True:\n",
        "        yield from [*COLS_40, *COLS_30, *COLS_20, *COLS_10, *COLS_05]\n",
        "\n",
        "\n",
        "def yield_unknown_speaker_color() -> Iterator[str]:\n",
        "    GRAYS = [\"#80868B\", \"#9AA0A6\", \"#BDC1C6\", \"#DADCE0\", \"#E8EAED\", \"#F1F3F4\"]\n",
        "    while True:\n",
        "        yield from GRAYS\n",
        "\n",
        "\n",
        "def color_for_voice_id_mapping(speakers: list[Speaker]) -> dict[str, str]:\n",
        "    known_speaker_color = yield_known_speaker_color()\n",
        "    unknown_speaker_color = yield_unknown_speaker_color()\n",
        "\n",
        "    mapping: dict[str, str] = {}\n",
        "    for speaker in speakers:\n",
        "        if speaker.name != UNKNOWN_DATA or speaker.role != UNKNOWN_DATA:\n",
        "            mapping[speaker.voice_id] = next(known_speaker_color)\n",
        "        else:\n",
        "            mapping[speaker.voice_id] = next(unknown_speaker_color)\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def get_table_styler(df: DataFrame) -> Styler:\n",
        "    def join_styles(styles: list[str]) -> str:\n",
        "        return \";\".join(styles)\n",
        "\n",
        "    table_css = [\n",
        "        \"color: #202124\",\n",
        "        \"border-collapse: collapse\",\n",
        "        \"border: solid 0.25em #BDC1C6\",\n",
        "        \"border-radius: 0.25em\",\n",
        "        \"outline: 0.25em solid #BDC1C6\",\n",
        "    ]\n",
        "    th_css = [\"background-color:#E8EAED\", \"text-align:center\"]\n",
        "    th_td_css = [\"padding:0.5ex 1ex\", \"text-align:left\"]\n",
        "    table_styles = [\n",
        "        CSSDict(selector=\"\", props=join_styles(table_css)),\n",
        "        CSSDict(selector=\"th\", props=join_styles(th_css)),\n",
        "        CSSDict(selector=\"th,td\", props=join_styles(th_td_css)),\n",
        "    ]\n",
        "\n",
        "    return df.style.set_table_styles(table_styles).hide()\n",
        "\n",
        "\n",
        "def display_speakers(transcription: VideoTranscription) -> None:\n",
        "    def sanitize_field(s: str, symbol_if_unknown: str) -> str:\n",
        "        return symbol_if_unknown if s == UNKNOWN_DATA else s\n",
        "\n",
        "    def yield_row() -> Iterator[list[str]]:\n",
        "        yield [\"voice_id\", \"name\", \"role\", \"company\"]\n",
        "        for speaker in transcription.step2_speakers:\n",
        "            name = sanitize_field(speaker.name, \"?\")\n",
        "            role = sanitize_field(speaker.role, \"?\")\n",
        "            company = sanitize_field(speaker.company, \"\")\n",
        "            yield [speaker.voice_id, name, role, company]\n",
        "\n",
        "    def speaker_bgcolor(row: Series) -> list[str]:\n",
        "        color = color_for_voice_id[row[\"voice_id\"]]\n",
        "        return [f\"background-color:{color}\"] * len(row)\n",
        "\n",
        "    data = yield_row()\n",
        "    color_for_voice_id = color_for_voice_id_mapping(transcription.step2_speakers)\n",
        "    columns = next(data)\n",
        "\n",
        "    df = DataFrame(columns=columns, data=data)\n",
        "    styler = get_table_styler(df)\n",
        "    styler.apply(speaker_bgcolor, axis=1)\n",
        "\n",
        "    display_markdown(f\"## Speakers ({len(transcription.step2_speakers)})\")\n",
        "    IPython.display.display(styler)\n",
        "\n",
        "\n",
        "def display_transcripts(transcription: VideoTranscription) -> None:\n",
        "    def yield_row() -> Iterator[list[str]]:\n",
        "        yield [\"voice_id\", \"timecode\", \"speaker\", \"transcript\"]\n",
        "\n",
        "        speaker_by_id = {\n",
        "            speaker.voice_id: speaker for speaker in transcription.step2_speakers\n",
        "        }\n",
        "        previous_voice_id = None\n",
        "        for transcript in transcription.step1_transcripts:\n",
        "            speaker = speaker_by_id.get(transcript.voice_id, None)\n",
        "            speaker_label = \"\"\n",
        "            if speaker:\n",
        "                if speaker.name != UNKNOWN_DATA:\n",
        "                    speaker_label = speaker.name\n",
        "                elif speaker.role != UNKNOWN_DATA:\n",
        "                    speaker_label = f\"[{speaker.role}]\"\n",
        "            if not speaker_label:\n",
        "                speaker_label = f\"[{transcript.voice_id}]\"\n",
        "            yield [\n",
        "                transcript.voice_id,\n",
        "                transcript.timecode,\n",
        "                speaker_label if transcript.voice_id != previous_voice_id else '\"',\n",
        "                transcript.verbatim,\n",
        "            ]\n",
        "            previous_voice_id = transcript.voice_id\n",
        "\n",
        "    def speaker_bgcolor(row: Series) -> list[str]:\n",
        "        color = color_for_voice_id[row[\"voice_id\"]]\n",
        "        speaker_bgcolor = f\"background-color:{color}\"\n",
        "        return [speaker_bgcolor] * len(row)\n",
        "\n",
        "    data = yield_row()\n",
        "    color_for_voice_id = color_for_voice_id_mapping(transcription.step2_speakers)\n",
        "    df = DataFrame(columns=next(data), data=data)\n",
        "\n",
        "    styler = get_table_styler(df)\n",
        "    styler.apply(speaker_bgcolor, axis=1)\n",
        "    styler.hide([\"voice_id\"], axis=\"columns\")\n",
        "\n",
        "    display_markdown(f\"## Transcripts ({len(transcription.step1_transcripts)})\")\n",
        "    IPython.display.display(styler)\n",
        "\n",
        "\n",
        "def transcribe_video(video: Video, video_segment: VideoSegment | None = None) -> None:\n",
        "    display_video(video)\n",
        "    transcription = get_video_transcription(video, video_segment)\n",
        "    display_speakers(transcription)\n",
        "    display_transcripts(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSgqZuLDqoN4"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ91H4VWYMQk"
      },
      "source": [
        "### üéûÔ∏è Short video with 6 speakers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcribe_video(TestVideo.GDM_PODCAST_TRAILER_0MIN_59S)"
      ],
      "metadata": {
        "id": "GPSuRE70Yhuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "josZMk6UqoN5"
      },
      "source": [
        "### üéûÔ∏è Video with no visible speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a0squK0qoN5"
      },
      "outputs": [],
      "source": [
        "transcribe_video(TestVideo.JANE_GOODALL_2MIN_42S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_xX8DHqoN5"
      },
      "source": [
        "### üéûÔ∏è French video with many speakers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYd_6Am0qoN5"
      },
      "outputs": [],
      "source": [
        "transcribe_video(TestVideo.BRUT_FR_DOGS_WATER_LEAK_8MIN_28S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeC7dVBsqoN5"
      },
      "source": [
        "### üéûÔ∏è English video with many speakers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NuJ8ZDLqoN5"
      },
      "outputs": [],
      "source": [
        "transcribe_video(TestVideo.GDM_ALPHAFOLD_7MIN_54S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaWO4vnqoN5"
      },
      "source": [
        "### üéûÔ∏è Long video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUawN12iqoN5"
      },
      "outputs": [],
      "source": [
        "transcribe_video(\n",
        "    TestVideo.GDM_VIEW_FROM_FRONTIER_34MIN_38S,\n",
        "    VideoSegment(\n",
        "        start=timedelta(seconds=0),\n",
        "        end=timedelta(minutes=19),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymRS4G5yqoN5"
      },
      "source": [
        "### üéûÔ∏è Test your own videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fK9OcMCqoN5"
      },
      "outputs": [],
      "source": [
        "class MyVideo(Video):\n",
        "    A_xMIN_yS = youtube_url_from_id(\"\")\n",
        "    B_xMIN_yS = \"gs://bucket/path/to/video.*\"\n",
        "    C_xMIN_yS = \"https://path/to/video.*\"\n",
        "    pass\n",
        "\n",
        "# transcribe_video(MyVideo.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}